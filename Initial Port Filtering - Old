## Last updated by Diran on 6/24/24

# Packages for Handling Data
import pandas as pd
import matplotlib.path as path
import numpy as np

# Packages for interfacing with internet
from io import BytesIO
from zipfile import ZipFile
from urllib.request import urlopen

# Packages for parallel processing
import concurrent.futures

# Package for timing - TEMPORARY
import time

### Creating All URLs ###

## Best data is from 2015 through 2023 (has the most recent / rigorous data collection standards)

# General URL format: https://coast.noaa.gov/htdata/CMSP/AISDataHandler/Year/AIS_Year_MonthNum_DayNum.zip

# First, construct a list of all the dates in the year as numbers:

days = ['0' + str(i) if i < 10 else str(i) for i in range(1,32)]

January = ['_01_' + i for i in days]
February = ['_02_' + i for i in days[:28]]
March = ['_03_' + i for i in days]
April = ['_04_' + i for i in days[:30]]
May = ['_05_' + i for i in days]
June = ['_06_' + i for i in days[:30]]
July = ['_07_' + i for i in days]
August = ['_08_' + i for i in days]
September = ['_09_' + i for i in days[:30]]
October = ['_10_' + i for i in days]
November = ['_11_' + i for i in days[:30]]
December = ['_12_' + i for i in days]
    # The underlines are to make the date fit into the final url
    
months = [January, February, March, April, May, June, July, August, September, November, December]

date_nums = [day for month in months for day in month]

# Next, the beginnning of each URL is the same for 2015 - 2023

base_url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/'

# Combine the date numbers with the general url format to get all URL's:

URLs = [base_url + str(Year) + '/AIS_' + str(Year) + Date + '.zip' for Date in date_nums for Year in range(2018, 2024)]
    # Our port data is only from 2018 onwards, hence we don't consider 2015-2017

# Can't forget about leap years!
URLs.append('https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2016/AIS_2016_02_29.zip')
URLs.append('https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/AIS_2020_02_29.zip')


### Filtering Data ###

# Information on what AIS information means:
    #https://documentation.spire.com/ais-fundamentals/different-classes-of-ais/ais-channel-access-methods/

# Read filters from the excel Sheet "Port_Boundaries" in this GitHub Repository    
port_data = pd.read_excel("https://raw.githubusercontent.com/DiranOrange/Key-Bridge-Code-and-Data/main/Port_Boundaries.xlsx", header = 0, index_col=0)

dict_ports = port_data.transpose().to_dict('list')

# Each port boundary is defined as a path of four vertices based on real world lat/lon data
port_boxes = {port: path.Path(np.array(dict_ports[port]).reshape(5, 2)) for port in dict_ports}

def filter_file(file, boundaries):
    """
    Parameters
    ----------
    file : path to the file the be filtered
        Zipfiles from the internet will be opened, and the path will be listed here
    
    boundaries : Dictionary of the form Location: Foo: Path Defining Foo's Boundaries
        For our purposes, Foo will likely be a port or bridge that we expect to see high traffic
        A boundary is a closed path
        The Path can consist of any number of pairs of lat/lon coordinates that serve as points / vertices
            A path object comes from matplotlib.path, which has useful tools (specifically, determining if a point is within a closed path)
    
    Returns
    -------
    None. Data is saved outside the program, thus nothing needs to be returned
        #In the event that data cannot be written to a non-local csv file, the filtered data will be returned by this function
    """
    
    #Each file is prepared as a data frame
    raw_data = pd.read_csv(file, sep=',', header = 0)
     
    # Filter data based on each port of interest
    for port in boundaries:
        
        # The below checks are visually complicated, but are simple in nature: 
            # Does the row have a boat big enough for us to care?
            # Does the row have a point with a port boundary?
        
        # .loc requires the use of "&" instead of "and" and "|" instead of "or"
            # Pay close attention to parenthesis and order of operations
        
        filtered_data = raw_data.loc[(((70 <= raw_data["VesselType"]) & (raw_data["VesselType"] < 90)) | (raw_data["VesselType"] == (1016 | 1017 | 1024 | 61)))
            # Vessel types 70-90 & 1016, 1017, 1024 consist of tankers and cargo ships. 61 is for cruise ships
                                     &
        (boundaries[port].contains_points(np.array([raw_data["LON"], raw_data["LAT"]]).transpose()))
            # This checks whether a ship is within a port boundary at the time of its AIS broadcast
                                     &
        (raw_data["SOG"] > 0.3)]
            # This checks whether the boat is moving or stationary
                # By trial and error, boats drifting in harbor don't seem to move faster than 0.3 knots                   
        
        # Put the data for each bridge into its own csv file
        filtered_data.to_csv(('D:\Marine Data' + '\\' + port + ' Data.csv'), mode = 'a')
            # WARNING: This filepath is temporary due to it being local, and will cause issues if not changed after downloading
            # TDL: Replace the local address with rockfish storage
    return


### Downloading the Data ###

# Go through each URL, open the file, then apply the filtering function    

def download_and_filter(url):
    # Download the file from the link so it can be interacted with
    download = urlopen(url)
    # Unzip the file
    data_zip_file = ZipFile(BytesIO(download.read()))
    
    #The actual file name is derived from the url, and looks like AIS_Year_DateNum
    file_name = url[55:-4] + '.csv'
        #The first 50 characters are the base url, the last 4 are .zip
    
    #Apply the filter function to the file
    filter_file(data_zip_file.open(file_name), boundaries=port_boxes)

### Running the code ###:
if __name__ == "__main__":
    
    # This process is IO bound due to the heavy network requirements (downloading thousands of datasets off the internet)
    # Thus threading is chosen to speed up the process
        
    with concurrent.futures.ThreadPoolExecutor() as executor : 
        executor.map(download_and_filter, URLs)
        
